# Global LLM configuration
[llm]
model = "claude-3-7-sonnet-20250219"        # The LLM model to use
base_url = "https://api.anthropic.com/v1/"  # API endpoint URL
api_key = "YOUR_API_KEY"                    # Your API key
max_tokens = 8192                           # Maximum number of tokens in the response
temperature = 0.0                           # Controls randomness
rpm_limit = 50                            # Optional: Maximum requests per minute (e.g., 10 RPM)
# tpm_limit = 4000000                       # Optional: Maximum tokens per minute (Claude doesn't have this)
itpm_limit = 20000                      # Optional: Maximum input tokens per minute (e.g., 20k ITPM)
otpm_limit = 8000                      # Optional: Maximum output tokens per minute (e.g., 8k OTPM)

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPOLYMENT_ID}"
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"
# rpm_limit = 10                  # Optional: Maximum requests per minute (e.g., 10 RPM)
# tpm_limit = 4000000             # Optional: Maximum tokens per minute (e.g., 4M TPM)
# itpm_limit = 2000000            # Optional: Maximum input tokens per minute (e.g., 2M ITPM)
# otpm_limit = 2000000            # Optional: Maximum output tokens per minute (e.g., 2M OTPM)

# [llm] #OLLAMA:
# api_type = 'ollama'
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0
# rpm_limit = 20                            # Optional: Maximum requests per minute for local models
# tpm_limit = 1000000                       # Optional: Maximum tokens per minute for local models
# itpm_limit = 500000                       # Optional: Maximum input tokens per minute for local models
# otpm_limit = 500000                       # Optional: Maximum output tokens per minute for local models

# Optional configuration for specific LLM models
[llm.vision]
model = "claude-3-7-sonnet-20250219"        # The vision model to use
base_url = "https://api.anthropic.com/v1/"  # API endpoint URL for vision model
api_key = "YOUR_API_KEY"                    # Your API key for vision model
max_tokens = 8192                           # Maximum number of tokens in the response
temperature = 0.0                           # Controls randomness for vision model
# rpm_limit = 5                             # Optional: Maximum requests per minute (lower for vision models)
# tpm_limit = 3000000                       # Optional: Maximum tokens per minute for vision models
# itpm_limit = 1500000                      # Optional: Maximum input tokens per minute for vision models
# otpm_limit = 1500000                      # Optional: Maximum output tokens per minute for vision models

# [llm.vision] #OLLAMA VISION:
# api_type = 'ollama'
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0
# rpm_limit = 10                            # Optional: Maximum requests per minute for local vision models
# tpm_limit = 800000                        # Optional: Maximum tokens per minute for local vision models
# itpm_limit = 400000                       # Optional: Maximum input tokens per minute for local vision models
# otpm_limit = 400000                       # Optional: Maximum output tokens per minute for local vision models

# Optional configuration for specific browser configuration
# [browser]
# Whether to run browser in headless mode (default: false)
#headless = false
# Disable browser security features (default: true)
#disable_security = true
# Extra arguments to pass to the browser
#extra_chromium_args = []
# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = ""
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
# [search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo".
#engine = "Google"

## Sandbox configuration
#[sandbox]
#use_sandbox = false
#image = "python:3.12-slim"
#work_dir = "/workspace"
#memory_limit = "1g"  # 512m
#cpu_limit = 2.0
#timeout = 300
#network_enabled = true

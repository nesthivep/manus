import json
import re
from typing import Any, List, Optional, Union

from pydantic import Field

from app.agent.react import ReActAgent
from app.exceptions import TokenLimitExceeded
from app.logger import logger
from app.prompt.toolcall import NEXT_STEP_PROMPT, SYSTEM_PROMPT
from app.schema import TOOL_CHOICE_TYPE, AgentState, Message, ToolCall, ToolChoice
from app.tool import CreateChatCompletion, Terminate, ToolCollection


TOOL_CALL_REQUIRED = "Tool calls required but none provided"


class ToolCallAgent(ReActAgent):
    """Base agent class for handling tool/function calls with enhanced abstraction"""

    name: str = "toolcall"
    description: str = "an agent that can execute tool calls."

    system_prompt: str = SYSTEM_PROMPT
    next_step_prompt: str = NEXT_STEP_PROMPT

    available_tools: ToolCollection = ToolCollection(
        CreateChatCompletion(), Terminate()
    )
    tool_choices: TOOL_CHOICE_TYPE = ToolChoice.AUTO  # type: ignore
    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])

    tool_calls: List[ToolCall] = Field(default_factory=list)

    max_steps: int = 30
    max_observe: Optional[Union[int, bool]] = None

    async def think(self) -> bool:
        """Process current state and decide next actions using tools"""
        if self.next_step_prompt:
            user_msg = Message.user_message(self.next_step_prompt)
            self.messages += [user_msg]

        try:
            # Get response with tool options
            response = await self.llm.ask_tool(
                messages=self.messages,
                system_msgs=[Message.system_message(self.system_prompt)]
                if self.system_prompt
                else None,
                tools=self.available_tools.to_params(),
                tool_choice=self.tool_choices,
            )
        except ValueError:
            raise
        except Exception as e:
            # Check if this is a RetryError containing TokenLimitExceeded
            if hasattr(e, "__cause__") and isinstance(e.__cause__, TokenLimitExceeded):
                token_limit_error = e.__cause__
                logger.error(
                    f"ðŸš¨ Token limit error (from RetryError): {token_limit_error}"
                )
                self.memory.add_message(
                    Message.assistant_message(
                        f"Maximum token limit reached, cannot continue execution: {str(token_limit_error)}"
                    )
                )
                self.state = AgentState.FINISHED
                return False
            raise

        self.tool_calls = response.tool_calls

        # Log response info
        logger.info(f"âœ¨ {self.name}'s thoughts: {response.content}")
        logger.info(
            f"ðŸ› ï¸ {self.name} selected {len(response.tool_calls) if response.tool_calls else 0} tools to use"
        )
        if response.tool_calls:
            logger.info(
                f"ðŸ§° Tools being prepared: {[call.function.name for call in response.tool_calls]}"
            )

        try:
            # Handle different tool_choices modes
            if self.tool_choices == ToolChoice.NONE:
                if response.tool_calls:
                    logger.warning(
                        f"ðŸ¤” Hmm, {self.name} tried to use tools when they weren't available!"
                    )
                if response.content:
                    self.memory.add_message(Message.assistant_message(response.content))
                    return True
                return False

            # Create and add assistant message
            assistant_msg = (
                Message.from_tool_calls(
                    content=response.content, tool_calls=self.tool_calls
                )
                if self.tool_calls
                else Message.assistant_message(response.content)
            )
            self.memory.add_message(assistant_msg)

            if self.tool_choices == ToolChoice.REQUIRED and not self.tool_calls:
                return True  # Will be handled in act()

            # For 'auto' mode, continue with content if no commands but content exists
            if self.tool_choices == ToolChoice.AUTO and not self.tool_calls:
                return bool(response.content)

            return bool(self.tool_calls)
        except Exception as e:
            logger.error(f"ðŸš¨ Oops! The {self.name}'s thinking process hit a snag: {e}")
            self.memory.add_message(
                Message.assistant_message(
                    f"Error encountered while processing: {str(e)}"
                )
            )
            return False

    async def act(self) -> str:
        """Execute tool calls and handle their results"""
        if not self.tool_calls:
            if self.tool_choices == ToolChoice.REQUIRED:
                raise ValueError(TOOL_CALL_REQUIRED)

            # Return last message content if no tool calls
            return self.messages[-1].content or "No content or commands to execute"

        results = []
        for command in self.tool_calls:
            result = await self.execute_tool(command)

            if self.max_observe:
                result = result[: self.max_observe]

            logger.info(
                f"ðŸŽ¯ Tool '{command.function.name}' completed its mission! Result: {result}"
            )

            # Add tool response to memory
            tool_msg = Message.tool_message(
                content=result, tool_call_id=command.id, name=command.function.name
            )
            self.memory.add_message(tool_msg)
            results.append(result)

        return "\n\n".join(results)

    async def execute_tool(self, command: ToolCall) -> str:
        """Execute a single tool call with robust error handling"""
        if not command or not command.function or not command.function.name:
            return "Error: Invalid command format"

        name = command.function.name
        if name not in self.available_tools.tool_map:
            return f"Error: Unknown tool '{name}'"

        try:
            # Parse arguments
            args = json.loads(command.function.arguments or "{}")

            # Execute the tool
            logger.info(f"ðŸ”§ Activating tool: '{name}'...")
            result = await self.available_tools.execute(name=name, tool_input=args)

            # Format result for display
            observation = (
                f"Observed output of cmd `{name}` executed:\n{str(result)}"
                if result
                else f"Cmd `{name}` completed with no output"
            )

            # Handle special tools like `finish`
            await self._handle_special_tool(name=name, result=result)

            return observation
        except json.JSONDecodeError:
            error_msg = f"Error parsing arguments for {name}: Invalid JSON format"
            logger.error(
                f"ðŸ“ Oops! The arguments for '{name}' don't make sense - invalid JSON, arguments:{command.function.arguments}"
            )
            return f"Error: {error_msg}"
        except Exception as e:
            error_msg = f"âš ï¸ Tool '{name}' encountered a problem: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"

    async def _handle_special_tool(self, name: str, result: Any, **kwargs):
        """Handle special tool execution and state changes"""
        if not self._is_special_tool(name):
            return

        if self._should_finish_execution(name=name, result=result, **kwargs):
            # Set agent state to finished
            logger.info(f"ðŸ Special tool '{name}' has completed the task!")
            self.state = AgentState.FINISHED

    @staticmethod
    def _should_finish_execution(self, name: str, result: Any, **kwargs) -> bool:
        """Determine if tool execution should finish the agent"""
        # Only terminate if explicitly called and task is truly complete
        if name.lower() != "terminate":
            return False

        # Get the original user request
        original_request = next(
            (msg.content for msg in self.memory.messages if msg.role == "user"), None
        )
        if not original_request:
            return True

        # Ask LLM to evaluate if the task is truly complete
        completion_check = self._evaluate_task_completion(original_request, result)
        if not completion_check.get("is_complete", True):
            # Add a reflection message to reconsider approach
            reflection = completion_check.get(
                "reflection", "Let's try a different approach to complete this task."
            )
            self.update_memory("system", reflection)
            return False

        return True

    async def _evaluate_task_completion(
        self, original_request: str, result: str
    ) -> dict:
        """Evaluate if the task is truly complete based on the original request"""
        # First summarize the recent conversation context
        recent_messages = self.memory.messages[-10:]

        # Get a summary of what's been done so far
        try:
            context_msgs = [
                msg for msg in recent_messages if msg.content and hasattr(msg, "role")
            ]
            if context_msgs:
                summary_prompt = f"""Summarize the key actions and findings from this conversation in 2-3 sentences.
                Focus on what tools were used and what was discovered or accomplished."""

                summary_response = await self.llm.ask(
                    messages=context_msgs + [Message.user_message(summary_prompt)],
                    system_msgs=[
                        Message.system_message(
                            "You create concise summaries of conversations."
                        )
                    ],
                )
                context_summary = summary_response.content
            else:
                context_summary = "No significant actions taken yet."
        except Exception as e:
            logger.error(f"Error summarizing context: {e}")
            context_summary = "Context summarization failed."

        # Now use the summary in the evaluation prompt
        evaluation_prompt = f"""
        Original request: {original_request}

        What's been done so far: {context_summary}

        Evaluate if this task is truly complete. Consider:
        1. Have all parts of the request been addressed?
        2. Was the execution thorough and comprehensive?
        3. Are there alternative approaches that could yield better results?
        4. Did we explore sufficiently or stop too early?

        Return JSON: {{"is_complete": boolean, "reflection": "guidance if incomplete"}}
        """

        try:
            response = await self.llm.ask(
                messages=[Message.user_message(evaluation_prompt)],
                system_msgs=[
                    Message.system_message(
                        "You are a critical evaluator determining if a task is truly complete."
                    )
                ],
            )

            # Extract JSON from response in case it's embedded in text
            json_match = re.search(r"\{.*\}", response.content, re.DOTALL)
            if json_match:
                result_json = json.loads(json_match.group(0))
                return result_json

            # Default to completing if parsing fails
            return {"is_complete": True}
        except Exception as e:
            logger.error(f"Error evaluating task completion: {e}")
            return {"is_complete": True}  # Default to completing on error

    def _is_special_tool(self, name: str) -> bool:
        """Check if tool name is in special tools list"""
        return name.lower() in [n.lower() for n in self.special_tool_names]
